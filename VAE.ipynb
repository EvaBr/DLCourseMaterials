{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from tensorflow.keras.layers import Dense, Activation, Add, Multiply, Input, Reshape, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#get data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype('float32') / 255.\n",
    "x_test = x_test.reshape(-1, 784).astype('float32') / 255.\n",
    "\n",
    "#x_train = np.expand_dims(x_train, axis=-1)\n",
    "#x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "#set parameters\n",
    "middle_layer = 512\n",
    "latent_size = 3\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "inp = Input(shape=(784,), name=\"input\")\n",
    "enc = Dense(middle_layer, activation='relu', name=\"encoding1\")(inp)\n",
    "enc_sigma = Dense(latent_size, name=\"encoding2_Sigma\")(enc)\n",
    "enc_mu = Dense(latent_size, name=\"encoding2_Mu\")(enc)\n",
    "#eps = Input(tensor=tfp.Normal().sample(latent_size))\n",
    "#tmp = Multiply()([enc_sigma, eps])\n",
    "#latent = Add()([enc_mu, tmp])\n",
    "concatenated_latent = Concatenate(axis=1,name=\"SigmaAndMu\")([enc_sigma, enc_mu])\n",
    "latent = Lambda(lambda ms: tfp.distributions.MultivariateNormalDiag(loc=ms[0], scale_diag=ms[1]).sample(), name=\"LatentEncoding\")([enc_mu, enc_sigma])\n",
    "\n",
    "encoder = Model(inputs=inp, outputs=latent, name=\"encoder\")\n",
    "\n",
    "inp_from_latent = Input(shape=[latent_size], name=\"input2\")\n",
    "dec = Dense(middle_layer, activation='relu', name=\"decoding1\")(inp_from_latent)\n",
    "dec = Dense(784, activation='relu', name=\"decoding2\")(dec)\n",
    "out_dist = Reshape([28, 28, 1], name=\"Reshape\")(dec)\n",
    "#out_dist_joint = Lambda(lambda decoded: tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=out_dist), 2), output_shape=(1,1), name=\"BernoulliProbability\")(dec)\n",
    "#we cant do the lambda layer above, because it wont return anything; just instantiates distro. \n",
    "#BUT, there exists an actual tfp layer!! So apparently, you CAN return a distribution! :)\n",
    "out_dist_joint = tfp.layers.IndependentBernoulli((28,28,1), tfp.distributions.Bernoulli.logits, name=\"Bernoulli\")\n",
    "\n",
    "\n",
    "decoder = Model(inputs=inp_from_latent, outputs=out_dist, name=\"decoder\")\n",
    "\n",
    "vae_model = Model(inputs=encoder.inputs, outputs=[concatenated_latent, out_dist_joint(decoder(latent))])#out_dist_joint(out_dist)])\n",
    "\n",
    "vae_model.summary()\n",
    "plot_model(vae_model, to_file='vae_model.png')\n",
    "\n",
    "#kl_loss = tf.reduce_mean(- 0.5 * tf.reduce_sum(1 + enc_sigma - tf.square(enc_mu) - tf.exp(enc_sigma), axis=-1))\n",
    "#reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(inp, dec) * 784) #vae_model.outputs[0].log_prob(inp), axis=-1)\n",
    "#vae_model.add_loss(kl_loss)\n",
    "#vae_model.add_loss(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoding1 (Dense)               (None, 512)          401920      input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Mu (Dense)                      (None, 3)            1539        encoding1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Sigma (Dense)                   (None, 3)            1539        encoding1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "LatentEncoding (Lambda)         (None, 3)            0           Mu[0][0]                         \n",
      "                                                                 Sigma[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 784)          404240      LatentEncoding[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 809,238\n",
      "Trainable params: 809,238\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "inp = Input(shape=(784,), name=\"input\")\n",
    "enc = Dense(middle_layer, activation='relu', name=\"encoding1\")(inp)\n",
    "enc_sigma = Dense(latent_size, name=\"Sigma\")(enc)\n",
    "enc_mu = Dense(latent_size, name=\"Mu\")(enc)\n",
    "#concatenated_latent = Concatenate(axis=1,name=\"SigmaAndMu\")([enc_sigma, enc_mu])\n",
    "#latent = Lambda(lambda ms: tfp.distributions.MultivariateNormalDiag(loc=ms[0], scale_diag=ms[1]).sample(sample_shape=tf.shape(enc_mu)), name=\"LatentEncoding\")([enc_mu, enc_sigma])\n",
    "latent = Lambda(lambda mu: mu[0] + tf.exp(0.5 * mu[1]) * tf.random_normal(shape=tf.shape(mu[0])), name=\"LatentEncoding\")((enc_mu, enc_sigma))\n",
    "\n",
    "encoder = Model(inputs=inp, outputs=[enc_mu, enc_sigma, latent])\n",
    "\n",
    "inp_from_latent = Input(shape=latent_size, name=\"input2\")\n",
    "dec1 = Dense(middle_layer, activation='relu', name=\"decoding1\")(inp_from_latent)\n",
    "dec = Dense(784, activation='sigmoid', name=\"decoding2\")(dec1)\n",
    "##out_dist = Reshape([28, 28], name=\"Reshape\")(dec)\n",
    "#out_dist_joint = tfp.layers.IndependentBernoulli([784], tfp.distributions.Bernoulli.logits, name=\"Bernoulli\")\n",
    "\n",
    "decoder = Model(inputs=inp_from_latent, outputs=dec)\n",
    "\n",
    "outs = decoder(latent)\n",
    "\n",
    "#vae_model = Model(inputs=inp, outputs=[enc_mu, enc_sigma, out_dist_joint(dec)], name=\"VAE\")\n",
    "vae_model = Model(inputs=inp, outputs=outs, name=\"VAE\")\n",
    "\n",
    "vae_model.summary()\n",
    "plot_model(vae_model, to_file='vae_model.png')\n",
    "\n",
    "\n",
    "\n",
    "kl_loss = tf.reduce_mean(- 0.5 * tf.reduce_sum(1 + enc_sigma - tf.square(enc_mu) - tf.exp(enc_sigma), axis=-1))\n",
    "reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(inp, outs) * 784) #vae_model.outputs[0].log_prob(inp), axis=-1)\n",
    "vae_model.add_loss(kl_loss)\n",
    "vae_model.add_loss(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need multiple losses:\n",
    "def reconstruction_loss(y_true, y_pred):\n",
    "    return - tf.reduce_mean(y_pred.log_prob(y_true), axis=-1)\n",
    "def KL_loss_sigma(y_true, y_pred):\n",
    "    return tf.reduce_mean(0.5*(-tf.reduce_sum(y_pred, axis=-1) + tf.reduce_sum(tf.exp(y_pred), axis=-1)))\n",
    "def KL_loss_mu(y_true, y_pred):\n",
    "    return tf.reduce_mean(0.5*(tf.reduce_sum(tf.square(y_pred), axis=-1) - tf.cast(tf.size(y_pred), tf.dtypes.float32)))\n",
    "\n",
    "dummy = np.zeros(latent_size)\n",
    "losses = {\"Bernoulli\" : reconstruction_loss,\n",
    "          \"Sigma\" : KL_loss_sigma,\n",
    "          \"Mu\" : KL_loss_mu}\n",
    "loss_weights = {\"Bernoulli\" : 1.0, \n",
    "                \"Sigma\" : 1.0,\n",
    "                \"Mu\" : 1.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 08:03:38.005712 140136269260608 training_utils.py:1101] Output model_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to model_3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 140.4374 - val_loss: 141.3794\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 139.8671 - val_loss: 141.2957\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 139.7739 - val_loss: 141.3593\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 139.6457 - val_loss: 141.0355\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 139.5367 - val_loss: 141.0671\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 139.3896 - val_loss: 140.8326\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 139.2839 - val_loss: 140.8319\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 139.1638 - val_loss: 140.7821\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 139.0324 - val_loss: 140.6728\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 138.9320 - val_loss: 140.5219\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 138.7789 - val_loss: 140.4363\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 138.6467 - val_loss: 140.3648\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 138.5644 - val_loss: 140.3471\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 138.4222 - val_loss: 140.2432\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 138.3158 - val_loss: 140.1933\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 138.2469 - val_loss: 139.9968\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 138.0924 - val_loss: 139.9743\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 138.0061 - val_loss: 139.8933\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 137.8714 - val_loss: 139.8691\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 137.7822 - val_loss: 139.7380\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 137.6756 - val_loss: 139.6323\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 137.5748 - val_loss: 139.5379\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 137.4825 - val_loss: 139.5011\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 137.3640 - val_loss: 139.5252\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 137.2958 - val_loss: 139.4536\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 137.1754 - val_loss: 139.3560\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 137.0883 - val_loss: 139.2764\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 137.0024 - val_loss: 139.3254\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 136.9109 - val_loss: 139.1594\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 136.8045 - val_loss: 139.1644\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 136.7213 - val_loss: 139.0636\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 136.6210 - val_loss: 138.9650\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 136.5323 - val_loss: 138.9302\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 136.4792 - val_loss: 138.8524\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 136.4137 - val_loss: 138.9172\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 136.3100 - val_loss: 138.9896\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 136.1994 - val_loss: 138.8189\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 136.1177 - val_loss: 138.7233\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 136.0508 - val_loss: 138.5906\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 135.9703 - val_loss: 138.5877\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 135.8866 - val_loss: 138.5455\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.8174 - val_loss: 138.5898\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.7508 - val_loss: 138.4785\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.6241 - val_loss: 138.5915\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 135.6051 - val_loss: 138.3002\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.4993 - val_loss: 138.3739\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 135.4181 - val_loss: 138.3208\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.3971 - val_loss: 138.4148\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.2964 - val_loss: 138.3485\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.2561 - val_loss: 138.1872\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 135.1480 - val_loss: 138.1724\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 135.1156 - val_loss: 138.2247\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 135.0609 - val_loss: 138.2412\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.9937 - val_loss: 138.0509\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.9016 - val_loss: 138.0391\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.8475 - val_loss: 138.0827\n",
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 134.7845 - val_loss: 138.0539\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.7387 - val_loss: 138.0709\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 134.6823 - val_loss: 137.9282\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 134.5898 - val_loss: 137.9400\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 134.5405 - val_loss: 137.8852\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 134.5012 - val_loss: 137.9515\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.4091 - val_loss: 137.8262\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 134.3674 - val_loss: 137.9238\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.3155 - val_loss: 137.9441\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.2479 - val_loss: 137.8159\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 134.2238 - val_loss: 137.7704\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 134.1617 - val_loss: 137.7421\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 134.1084 - val_loss: 137.6740\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 134.0661 - val_loss: 137.7344\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 133.9779 - val_loss: 137.8060\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 133.9388 - val_loss: 137.6104\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 64us/sample - loss: 133.9229 - val_loss: 137.5895\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 133.8351 - val_loss: 137.6301\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 133.7934 - val_loss: 137.6317\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 133.7654 - val_loss: 137.5724\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 133.6660 - val_loss: 137.5248\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 133.6693 - val_loss: 137.5094\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 133.6360 - val_loss: 137.7851\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 133.5731 - val_loss: 137.5996\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 133.5097 - val_loss: 137.5235\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 133.4343 - val_loss: 137.5202\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 133.3833 - val_loss: 137.5046\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 133.3881 - val_loss: 137.4793\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 133.3196 - val_loss: 137.6683\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 133.3158 - val_loss: 137.5819\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 133.2519 - val_loss: 137.4750\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 133.2188 - val_loss: 137.4554\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 133.1887 - val_loss: 137.6140\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 133.1236 - val_loss: 137.4615\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 133.1044 - val_loss: 137.5632\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 133.0851 - val_loss: 137.3572\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 132.9968 - val_loss: 137.4577\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 132.9843 - val_loss: 137.2947\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.9677 - val_loss: 137.3947\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 132.9049 - val_loss: 137.3347\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.8583 - val_loss: 137.3454\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 132.7975 - val_loss: 137.3548\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 132.8083 - val_loss: 137.3202\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 132.6978 - val_loss: 137.2389\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 132.7125 - val_loss: 137.3959\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 132.6909 - val_loss: 137.4407\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 132.6528 - val_loss: 137.3457\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 132.5693 - val_loss: 137.2903\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.5612 - val_loss: 137.3280\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 132.5447 - val_loss: 137.2624\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 132.5200 - val_loss: 137.2222\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 132.4526 - val_loss: 137.2364\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 132.4245 - val_loss: 137.2783\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 132.4236 - val_loss: 137.2121\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 132.3727 - val_loss: 137.2275\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 132.2935 - val_loss: 137.3125\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 132.3058 - val_loss: 137.1658\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.2497 - val_loss: 137.1337\n",
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.2697 - val_loss: 137.2664\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.1995 - val_loss: 137.2553\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 132.1761 - val_loss: 137.1706\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 132.1407 - val_loss: 137.2123\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 132.1002 - val_loss: 137.3177\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.0811 - val_loss: 137.0344\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.0817 - val_loss: 137.3342\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 132.0457 - val_loss: 137.1862\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.9924 - val_loss: 137.2181\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.9894 - val_loss: 137.1807\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.9167 - val_loss: 137.3001\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 131.9082 - val_loss: 137.1356\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.8628 - val_loss: 137.0588\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.8168 - val_loss: 137.1376\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.7717 - val_loss: 137.1178\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.7759 - val_loss: 137.0641\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 131.7824 - val_loss: 137.0577\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 131.7611 - val_loss: 137.3138\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 131.6869 - val_loss: 137.0422\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 131.6251 - val_loss: 137.0500\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 131.6479 - val_loss: 137.1183\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 131.6369 - val_loss: 137.1577\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 131.5767 - val_loss: 137.0907\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 131.5526 - val_loss: 137.1421\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 131.5605 - val_loss: 137.0344\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.5072 - val_loss: 137.2124\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 131.5116 - val_loss: 137.1255\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 131.5040 - val_loss: 137.0668\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 131.4575 - val_loss: 137.2851\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 131.3914 - val_loss: 137.0571\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 66us/sample - loss: 131.3942 - val_loss: 137.0852\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 131.3402 - val_loss: 137.1747\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 131.3248 - val_loss: 137.0938\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 131.3047 - val_loss: 137.2435\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 131.3234 - val_loss: 137.3169\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 131.2678 - val_loss: 137.1765\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 131.2396 - val_loss: 137.0358\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 131.2004 - val_loss: 137.0734\n",
      "Epoch 153/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 131.1585 - val_loss: 137.1825\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 131.1482 - val_loss: 137.1908\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.1483 - val_loss: 137.0612\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 131.1146 - val_loss: 137.1858\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 131.1357 - val_loss: 137.1371\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 131.0861 - val_loss: 137.1033\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 131.0457 - val_loss: 137.1387\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 131.0342 - val_loss: 137.0458\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.9880 - val_loss: 137.0904\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.9615 - val_loss: 137.1223\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.9623 - val_loss: 137.2042\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 130.9735 - val_loss: 137.0609\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.9282 - val_loss: 137.0296\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.8900 - val_loss: 137.1048\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 130.8770 - val_loss: 137.0810\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 130.8803 - val_loss: 137.1137\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 130.8335 - val_loss: 137.1616\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 130.8421 - val_loss: 137.1298\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 130.7877 - val_loss: 136.9673\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 130.7824 - val_loss: 137.1297\n",
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.7599 - val_loss: 137.2065\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 130.6997 - val_loss: 137.0962\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 130.6886 - val_loss: 137.2453\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 130.6723 - val_loss: 137.2127\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 130.6860 - val_loss: 137.0789\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 130.6904 - val_loss: 137.0499\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 130.6368 - val_loss: 137.1341\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 130.6173 - val_loss: 137.2524\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.5981 - val_loss: 137.1142\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 130.5720 - val_loss: 137.1731\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 130.5418 - val_loss: 137.2155\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.5064 - val_loss: 137.1092\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.4920 - val_loss: 137.1969\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.4758 - val_loss: 137.1607\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.4884 - val_loss: 137.2131\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.4236 - val_loss: 137.2123\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.4283 - val_loss: 137.1527\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.3881 - val_loss: 136.9618\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.3835 - val_loss: 137.1684\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.3846 - val_loss: 137.1019\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.3455 - val_loss: 137.2347\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.3308 - val_loss: 137.1561\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.2773 - val_loss: 137.2158\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.2888 - val_loss: 137.0158\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.2744 - val_loss: 137.3369\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.2845 - val_loss: 137.2909\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 130.2671 - val_loss: 137.1969\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 130.2647 - val_loss: 137.1493\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "#vae_model.compile(optimizer=opt, loss=losses, loss_weights=loss_weights, metrics=[\"accuracy\"])\n",
    "vae_model.compile(optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "calb = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=15, restore_best_weights=True)\n",
    "\n",
    "\n",
    "#hist = vae_model.fit(x_train, {\"Bernoulli\": x_train, \"Sigma\": x_train, \"Mu\": x_train},\n",
    "#validation_data=(x_test, {\"Bernoulli\": x_test, \"Sigma\": x_test, \"Mu\": x_test}), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "hist = vae_model.fit(x_train, None, validation_data=(x_test, None), epochs=200, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, what='loss', title=\"\"):\n",
    "    # Visualize loss history\n",
    "    epoch_count = [i+1 for i in range(len(history[what]))]\n",
    "    plt.plot(epoch_count, history[what], 'r--')\n",
    "    if ('val_'+what) in history:\n",
    "        plt.plot(epoch_count, history['val_'+what], 'b-')\n",
    "        plt.legend(['Training '+what, 'Validation '+what])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(what.capitalize())\n",
    "    plt.title(title.capitalize())\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing loss and generating images:\n",
    "x_out = vae_model.predict(x_test)\n",
    "mu, sigma, x_latent = encoder.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
